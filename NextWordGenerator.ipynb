{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnupriya-ms/My-Projects/blob/main/NextWordGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6I6crYH_VPs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/email_spam.csv\")\n",
        "df_text=list(df.text.values)\n",
        "text=\" \".join(df_text) # joins all the text values from the df_text list into a single string, separated by a space."
      ],
      "metadata": {
        "id": "a9_4uupWs869"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing the text to create a sequence of words\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words=len(tokenizer.word_index) + 1  #calculates the total number of unique words in the vocabulary"
      ],
      "metadata": {
        "id": "lxUzPcvaA6hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences. The texts_to_sequences() method converts a list of texts into a list of sequences of integers\n",
        "input_sequences=[]\n",
        "for line in text.split('\\n'):\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0] #tokenizes the text into a sequence of integers using the previously initialized tokenizer.\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence= token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "zD7FARqxBrQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input sequences are padded to ensure that all sequences have the same length.\n",
        "max_seq_len=max([len(seq) for seq in input_sequences])\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_seq_len,padding='pre'))"
      ],
      "metadata": {
        "id": "TOSYyfKKEj0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=input_sequences[:,:-1]  #>>>>>except the last column\n",
        "y=input_sequences[:,-1]   #>>>>>Last column"
      ],
      "metadata": {
        "id": "jluYnCOiE1nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting each target vector as a binary vector\n",
        "y=np.array(tf.keras.utils.to_categorical(y,num_classes=total_words))"
      ],
      "metadata": {
        "id": "R_I2Nh7tGGSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()  #A sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
        "model.add(Embedding(total_words,100,input_length=max_seq_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words,activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-D2ow2eGjeS",
        "outputId": "6b8389e9-c521-4b7d-c19c-d3e5e0336e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 169, 100)          296900    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2969)              448319    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 895819 (3.42 MB)\n",
            "Trainable params: 895819 (3.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(learning_rate=0.01),metrics=[\"accuracy\"])\n",
        "model.fit(X,y,batch_size=128, epochs=30, shuffle=True)  #batch size==>Specifies the number of samples per gradient update."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD7Z0fA7v-9w",
        "outputId": "3f23dfd4-c3da-4d86-bd7e-7e47b09cddb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "85/85 [==============================] - 68s 731ms/step - loss: 7.0534 - accuracy: 0.0294\n",
            "Epoch 2/30\n",
            "85/85 [==============================] - 62s 736ms/step - loss: 6.7186 - accuracy: 0.0386\n",
            "Epoch 3/30\n",
            "85/85 [==============================] - 63s 738ms/step - loss: 6.4411 - accuracy: 0.0528\n",
            "Epoch 4/30\n",
            "85/85 [==============================] - 63s 740ms/step - loss: 6.0618 - accuracy: 0.0784\n",
            "Epoch 5/30\n",
            "85/85 [==============================] - 62s 734ms/step - loss: 5.6076 - accuracy: 0.1119\n",
            "Epoch 6/30\n",
            "85/85 [==============================] - 63s 736ms/step - loss: 5.1262 - accuracy: 0.1484\n",
            "Epoch 7/30\n",
            "85/85 [==============================] - 62s 731ms/step - loss: 4.6312 - accuracy: 0.1918\n",
            "Epoch 8/30\n",
            "85/85 [==============================] - 63s 742ms/step - loss: 4.1418 - accuracy: 0.2462\n",
            "Epoch 9/30\n",
            "85/85 [==============================] - 64s 750ms/step - loss: 3.6589 - accuracy: 0.3037\n",
            "Epoch 10/30\n",
            "85/85 [==============================] - 64s 751ms/step - loss: 3.1925 - accuracy: 0.3736\n",
            "Epoch 11/30\n",
            "85/85 [==============================] - 63s 743ms/step - loss: 2.7608 - accuracy: 0.4346\n",
            "Epoch 12/30\n",
            "85/85 [==============================] - 63s 741ms/step - loss: 2.3701 - accuracy: 0.5158\n",
            "Epoch 13/30\n",
            "85/85 [==============================] - 63s 744ms/step - loss: 2.0073 - accuracy: 0.5911\n",
            "Epoch 14/30\n",
            "85/85 [==============================] - 63s 746ms/step - loss: 1.6791 - accuracy: 0.6703\n",
            "Epoch 15/30\n",
            "85/85 [==============================] - 63s 741ms/step - loss: 1.3938 - accuracy: 0.7379\n",
            "Epoch 16/30\n",
            "85/85 [==============================] - 62s 735ms/step - loss: 1.1450 - accuracy: 0.7946\n",
            "Epoch 17/30\n",
            "85/85 [==============================] - 63s 737ms/step - loss: 0.9350 - accuracy: 0.8399\n",
            "Epoch 18/30\n",
            "85/85 [==============================] - 63s 737ms/step - loss: 0.7736 - accuracy: 0.8728\n",
            "Epoch 19/30\n",
            "85/85 [==============================] - 63s 747ms/step - loss: 0.6383 - accuracy: 0.8975\n",
            "Epoch 20/30\n",
            "85/85 [==============================] - 62s 733ms/step - loss: 0.5272 - accuracy: 0.9138\n",
            "Epoch 21/30\n",
            "85/85 [==============================] - 63s 742ms/step - loss: 0.4495 - accuracy: 0.9253\n",
            "Epoch 22/30\n",
            "85/85 [==============================] - 64s 753ms/step - loss: 0.3842 - accuracy: 0.9332\n",
            "Epoch 23/30\n",
            "85/85 [==============================] - 64s 750ms/step - loss: 0.3357 - accuracy: 0.9417\n",
            "Epoch 24/30\n",
            "85/85 [==============================] - 64s 752ms/step - loss: 0.3045 - accuracy: 0.9433\n",
            "Epoch 25/30\n",
            "85/85 [==============================] - 63s 740ms/step - loss: 0.2715 - accuracy: 0.9466\n",
            "Epoch 26/30\n",
            "85/85 [==============================] - 64s 750ms/step - loss: 0.2575 - accuracy: 0.9455\n",
            "Epoch 27/30\n",
            "85/85 [==============================] - 64s 748ms/step - loss: 0.2418 - accuracy: 0.9487\n",
            "Epoch 28/30\n",
            "85/85 [==============================] - 64s 752ms/step - loss: 0.2346 - accuracy: 0.9478\n",
            "Epoch 29/30\n",
            "85/85 [==============================] - 64s 750ms/step - loss: 0.2292 - accuracy: 0.9494\n",
            "Epoch 30/30\n",
            "85/85 [==============================] - 63s 745ms/step - loss: 0.2238 - accuracy: 0.9482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7861bca6b6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the next word\n",
        "def generate_next_word(model, tokenizer, max_seq_len, seed_text, num_next_words):\n",
        "    for _ in range(num_next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Seed text for prediction\n",
        "seed_text = \"You are among \"\n",
        "\n",
        "# Number of words to predict\n",
        "num_next_words = 5\n",
        "\n",
        "# Generate next word\n",
        "next_word = generate_next_word(model, tokenizer, max_seq_len, seed_text, num_next_words)\n",
        "print(\"Predicted next word:\", next_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RryktONizL97",
        "outputId": "0906a320-ebf4-4e90-8cfe-78c0f66fe3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: You are among  excited to do something stupid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the next word\n",
        "def generate_next_word(model, tokenizer, max_seq_len, seed_text, num_next_words):\n",
        "    for _ in range(num_next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Seed text for prediction\n",
        "seed_text = \"Have you\"\n",
        "\n",
        "# Number of words to predict\n",
        "num_next_words = 8\n",
        "\n",
        "# Generate next word\n",
        "next_word = generate_next_word(model, tokenizer, max_seq_len, seed_text, num_next_words)\n",
        "print(\"Predicted next word:\", next_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmqlPHz6zT3s",
        "outputId": "7a86bec1-f113-418d-e504-d75da581a35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: Have you been successful in winning projects and earning more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4# Define the chatbot function\n",
        "def chatbot():\n",
        "    print(\"Welcome to the Chatbot! Type 'exit' to end the conversation.\")\n",
        "\n",
        "    # Main loop to receive input and generate responses\n",
        "    while True:\n",
        "        # Receive input from the user\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check for exit condition\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Generate response\n",
        "        response = generate_next_word(model, tokenizer, max_seq_len, user_input, num_next_words=5)\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc4jKPaO4zZ_",
        "outputId": "92db4d3c-ee8b-47b5-9b0f-9eb8b084e9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Chatbot! Type 'exit' to end the conversation.\n",
            "You: the name of\n",
            "Chatbot: the name of your account is amazon and\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}